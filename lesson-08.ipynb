{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 08: Reinforcement Learning and $k$-Armed Bandit Problems\n",
    "\n",
    "## Objectives\n",
    "  - Introduce and define reinforcement learning terminology:\n",
    "    - timestep $t$\n",
    "    - actions $A_t$ and rewards $R_t$\n",
    "    - value $q_*(a)$ and estimated value $Q_t(a)$\n",
    "    - greedy actions, exploration vs. exploitation\n",
    "  - Compare Greedy vs. $\\epsilon$-Greedy action selection in $k$-Armed Bandit Problems\n",
    "  \n",
    "## Prerequisites\n",
    "  - You should have the following python packages installed:\n",
    "    - [matplotlib](http://matplotlib.org/index.html)\n",
    "    - [numpy](http://www.scipy.org/scipylib/download.html)\n",
    "    - [pandas](http://pandas.pydata.org/getpandas.html)\n",
    "    - [seaborn](http://seaborn.pydata.org/installing.html)\n",
    "    \n",
    "## Acknowledgements\n",
    "The code from this notebook is adapted from [Shangtong Zhang's python code](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction) that accompanies Chapter 2 of Sutton and Barto's textbook [Reinforcement Learning: an Introduction](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html). Shangtong Zhang's copyright notice is in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C) 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)    #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "As usual, we start by importing some useful libraries and modules. Don't worry if you get a warning message when importing `matplotlib` -- it just needs to build the font cache, and the warning is just to alert you that this may take a while the first time the cell is run.\n",
    "\n",
    "**Run** the cell below to import useful libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    print(\"Could not import numpy!\")\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "try:\n",
    "    # Import the seaborn Python visualization library\n",
    "    import seaborn as sns\n",
    "    print(\"Successfully imported seaborn! (Version {})\".format(sns.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import seaborn!\")\n",
    "    \n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('ggplot')\n",
    "    print(\"Successfully imported matplotlib.pyplot! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import matplotlib.pyplot!\")\n",
    "    \n",
    "try:\n",
    "    from IPython.display import display\n",
    "    print(\"Successfully imported display from IPython.display!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $k$-Armed Bandit Problem\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"http://www.clker.com/cliparts/5/y/Q/h/g/d/slot-machine-hi.png\" alt=\"Slot Machine\" style=\"background-color:red;width:120px\" /></td>\n",
    "<td><img src=\"http://www.clker.com/cliparts/5/y/Q/h/g/d/slot-machine-hi.png\" alt=\"Slot Machine\" style=\"background-color:orange;width:120px\" /></td>\n",
    "<td><img src=\"http://www.clker.com/cliparts/5/y/Q/h/g/d/slot-machine-hi.png\" alt=\"Slot Machine\" style=\"background-color:yellow;width:120px\" /></td>\n",
    "<td><img src=\"http://www.clker.com/cliparts/5/y/Q/h/g/d/slot-machine-hi.png\" alt=\"Slot Machine\" style=\"background-color:green;width:120px\" /></td>\n",
    "<td><img src=\"http://www.clker.com/cliparts/5/y/Q/h/g/d/slot-machine-hi.png\" alt=\"Slot Machine\" style=\"background-color:blue;width:120px\" /></td>\n",
    "<td><img src=\"http://www.clker.com/cliparts/5/y/Q/h/g/d/slot-machine-hi.png\" alt=\"Slot Machine\" style=\"background-color:purple;width:120px\" /></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Suppose we're at the Reinforcement Learning Casino and we see $k$ different slot machines. During a **time step** $t$, we can select as our **action** $A_t$ one of the $k$ slot machines. We then pull the lever and receive a payout or **reward** $R_t$. Each slot machine pays out winnings based on some **stationary probability distribution**, meaning the likelihood of a certain outcome or reward is **independent** of the current time step. Now we'll keep doing this for many many timesteps (we've got the whole day to spend in the Reinforcement Learning Casino). At each time step, we're faced with the question \"Which slot machine should we play to maximize our winnings?\" ... how do we answer this question?\n",
    "\n",
    "If we were omniscient beings, we would know the **value** $q_*(a)$ of each action $a$. The value $q_*(a)$ of action $a$ is defined as the **expected value** of the reward $R_t$, given that action $a$ is selected at time $t$:\n",
    "\n",
    "$$q_*(a)=\\mathbb{E}\\left(R_t\\vert A_t=a\\right).$$\n",
    "\n",
    "Knowing the value of all actions, we would also know which of the actions is the **optimal action**, or the one which has the greatest value $q_*$. However, we're not omniscient beings, so we'll have to keep track of our observations of actions and rewards. At each time step, we can estimate the value of each action. We'll denote the **estimated value** of action $a$ at time step $t$ as $Q_t(a)$. But how do we compute $Q_t(a)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Estimating the Value of Actions\n",
    "After some time at the Reinforcement Learning Casino, we want the estimated values $Q_t(a)$ for each action $a$. One approach is the **sample-average method**:\n",
    "\n",
    "$$Q_t(a)\\ \\dot{=}\\ \\frac{\\text{Sum of rewards when }a\\text{ is taken prior to }t}{\\text{Number of times }a\\text{ is taken prior to }t}$$\n",
    "\n",
    "Basically, we just add up all rewards from selecting action $a$, and divide by the total number of time steps during which action $a$ was selected. It's just the **average** of the **sample** of relevant rewards, hence the name **sample-average method**. A couple points to note:\n",
    "\n",
    "  - The sample-average method is not the only way to obtain the estimated value $Q_t(a)$, nor is it always the best way.\n",
    "  - If action $a$ has not yet been taken prior to time step $t$, the denominator in the sample-average method will be zero. In this case, we can define $Q_t(a)$ to be some default value, typically $Q_t(a)=0$.\n",
    "  \n",
    "Let's see the sample-average method in action. **Run** the cell below to create a $k$-Armed Bandit Problem with 6 arms (6 actions). Each of these six actions $a$ will have a value $q_*(a)$ drawn from the Normal Distribution $N(0,1)$ with mean 0 and variance 1. We'll choose actions randomly from the set of six arms for the first ten timesteps. The reward $R_t(A_t)$ for each action will be the value $q_*(A_t=a)$, plus a randomly-drawn value from the Normal Distribution $N(0,1)$. The output will be a `DataFrame` object with the timesteps, along with the action and reward for each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of arms (actions) for the k-Armed Bandit Problem\n",
    "k = 6\n",
    "\n",
    "# How many timesteps or actions to simulate\n",
    "num_timesteps = 10\n",
    "\n",
    "# Initialize the random seed for np.random methods\n",
    "np.random.seed(0)\n",
    "\n",
    "# Random choice of actions for the first num_timesteps actions\n",
    "actions = np.random.choice(range(k),num_timesteps,replace=True)\n",
    "\n",
    "# Initialize the rewards to be an array of zeroes\n",
    "rewards = np.zeros(num_timesteps)\n",
    "\n",
    "# Each of the k actions will have a random value drawn from\n",
    "# the normal distribution with mean zero and variance one.\n",
    "bandit_means = np.random.randn(k)\n",
    "\n",
    "# Fill the rewards array based on the bandit_means array,\n",
    "# but add random values drawn from the normal distribution N(0,1)\n",
    "for idx, action in enumerate(actions):\n",
    "    rewards[idx] = bandit_means[action] + np.random.randn()\n",
    "\n",
    "# Combine the timestep, action, and reward history into a DataFrame\n",
    "bandit_df = pd.DataFrame({'timestep':np.arange(num_timesteps),\n",
    "                          'action' :actions,\n",
    "                          'reward' :rewards},\n",
    "                         columns=['timestep','action','reward'])\n",
    "display(bandit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **run** the cell below to use the sample-average method to estimate the value $Q_t(a)$ of all six actions after ten time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(bandit_df\\\n",
    "        .groupby('action')\\\n",
    "        .mean()\\\n",
    "        .sort_values(by='reward',ascending=False)\\\n",
    "        ['reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like action 0 has the largest estimate, followed by action 4, action 3, action 2, action 5, and lastly, action 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting an Action - Let's be Greedy!\n",
    "\n",
    "So we've covered how to use the sample-average method to estimate the value $Q_t(a)$ of an action $a$, based on the history of past actions and rewards prior to timestep $t$. At any timestep, there is at least one action $a$ with the largest estimated value $Q_t(a)$. These actions are called **greedy** actions. Now we have a choice: we can either **exploit** our current knowledge and select the greedy action, or we can **explore** by selecting a non-greedy action. Exploring allows us to improve our estimates for the non-greedy actions (and perhaps discover a better action to exploit at later timesteps), while exploitation maximizes the immediate expected reward for the next step. We cannot both explore and exploit during the same timestep; for that reason, people often refer to the **conflict** of exploration vs. exploitation.\n",
    "\n",
    "The **greedy action selection** method chooses the action $a$ with the largest estimated value $Q_t(a)$ at each time step $t$:\n",
    "\n",
    "$$A_t\\ \\dot{=}\\ \\underset{a}{\\arg\\max}\\ Q_t(a)$$\n",
    "\n",
    "Looking at the action and reward history for the sample $k$-Armed Bandit Problem generated above, the greedy action selection method suggests we should select action 0. We can see how good of a choice this action really is. **Run** the cell below to display the true values, sorted with the optimal action on top. Did greedy action selection choose the optimal action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_values = pd.Series(data = bandit_means, name = \"values\", index=pd.Series(np.arange(k),name=\"action\"))\n",
    "display(true_values.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action 0 is actually ranked second! In this case, greedy action selection misses out on action 5, which has the greatest value $q_*(a)$. In fact, based on the sample-average method, action 5 is currently ranked *fifth* out of six! With only exploitation and no exploration, we will miss out on the optimal action!\n",
    "\n",
    "Our history of actions and rewards was only ten -- it shouldn't be a surprise that the greedy action selection method couldn't find the optimal action. However, by the [**law of large numbers**](https://en.wikipedia.org/wiki/Law_of_large_numbers), if we go for many many timesteps and take a much larger sample of actions and rewards, the estimated values $Q_t(a)$ should approach the true values $q_*(a)$ for each action $a$. **Run** the cell below to do exactly that: take 10,000 actions and receive 10,000 rewards. A [violin plot](http://seaborn.pydata.org/generated/seaborn.violinplot.html) will depict the reward distributions for each action (Note: you will need to have installed the [seaborn Python visualization library](http://seaborn.pydata.org/index.html) to generate the violin plot). For this much larger history of actions and rewards, do the reward distributions more closely match the true values from the above cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many timesteps or actions to simulate\n",
    "large_num_timesteps = 10000\n",
    "\n",
    "# Reset the random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Random choice of actions for the first num_timesteps actions\n",
    "large_actions = np.random.choice(range(k),large_num_timesteps,replace=True)\n",
    "\n",
    "# Initialize the rewards to be an array of zeroes\n",
    "large_rewards = np.zeros(large_num_timesteps)\n",
    "\n",
    "# Fill the rewards array based on the bandit_means array,\n",
    "# but add random values drawn from the normal distribution N(0,1)\n",
    "for idx, action in enumerate(large_actions):\n",
    "    large_rewards[idx] = bandit_means[action] + np.random.randn()\n",
    "    \n",
    "# Combine the timestep, action, and reward history into a DataFrame\n",
    "large_bandit_df = pd.DataFrame({'timestep':np.arange(large_num_timesteps),\n",
    "                                 'action' :large_actions,\n",
    "                                 'reward' :large_rewards},\n",
    "                                 columns=['timestep','action','reward'])\n",
    "\n",
    "# Create the violin plot, depicting the reward distribution of each action\n",
    "ax = sns.violinplot(x=\"action\", y=\"reward\",data=large_bandit_df)\n",
    "\n",
    "# Set the title of the violin plot\n",
    "title = ax.set_title(\"Distribution of Rewards for {}-Armed Bandit Problem\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's be $\\epsilon$-Greedy!\n",
    "\n",
    "A simple alternative to greedy action selection is the **$\\boldsymbol{\\epsilon}$-greedy action selection** method. Here, $\\epsilon$ is some number between 0 and 1, denoting the probability of choosing an action randomly, *i.e.* taking an *exploration* action. If we don't *explore*, then we *exploit*: the probability that the greedy action will be selected is $1-\\epsilon$.\n",
    "\n",
    "Does $\\epsilon$-greedy action selection pay off in the long-term? What about for different values of $\\epsilon$? Let's find out! Read through the cell below, coded by Shangtong Zhang (see Acknowledgements), to get an idea of what is contained within the `Bandit` class. The key methods are:\n",
    "  - `getAction(self)`: determining the next action based on the action-reward history, and whether to explore or exploit.\n",
    "  - `takeAction(self,action)`: take the prescribed `action`, receive the reward, and update the estimated values $Q_t(a)$ for each action for the next timestep.\n",
    "  \n",
    "Once you've read through the code, **run** the cell below to create the `Bandit` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    # @kArm: Number of arms\n",
    "    # @epsilon: probability for exploration in epsilon-greedy algorithm (0 <= epsilon <= 1)\n",
    "    # @initial: initial estimation for each action\n",
    "    # @stepSize: constant step size for updating estimations\n",
    "    # @sampleAverages: if True, use sample averages to update estimations instead of constant step size\n",
    "    # @UCBParam: if not None, use UCB algorithm to select action\n",
    "    # @gradient: if True, use gradient based bandit algorithm\n",
    "    # @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm\n",
    "    def __init__(self, kArm=10, epsilon=0, initial=0, stepSize=0.1,\\\n",
    "                 sampleAverages=False, UCBParam=None,\\\n",
    "                 gradient=False, gradientBaseline=False, trueReward=0):\n",
    "        # Number of arms for the k-Armed Bandit Problem\n",
    "        self.k = kArm\n",
    "        \n",
    "        # Step size used in the gradient based bandit algorithm\n",
    "        self.stepSize = stepSize\n",
    "        \n",
    "        # if True, use sample averages to update estimations instead of constant step size\n",
    "        self.sampleAverages = sampleAverages\n",
    "        \n",
    "        # The indices of all k bandits range from 0 to k-1\n",
    "        # self.indices is an array with all the bandit indices\n",
    "        self.indices = np.arange(self.k)\n",
    "        \n",
    "        # Initialize the current timestep: time = 0\n",
    "        self.time = 0\n",
    "        \n",
    "        # if not None, use Upper-Confidence-Bound (UCB) algorithm to select action\n",
    "        self.UCBParam = UCBParam\n",
    "        \n",
    "        # if True, use gradient based bandit algorithm\n",
    "        self.gradient = gradient\n",
    "        \n",
    "        # if True, use average reward as baseline for gradient based bandit algorithm\n",
    "        self.gradientBaseline = gradientBaseline\n",
    "        self.averageReward = 0\n",
    "        self.trueReward = trueReward\n",
    "\n",
    "        # real reward for each action\n",
    "        self.qTrue = []\n",
    "\n",
    "        # estimation for each action\n",
    "        self.qEst = np.zeros(self.k)\n",
    "\n",
    "        # Number of times each action has been chosen\n",
    "        self.actionCount = []\n",
    "        \n",
    "        # probability for exploration in epsilon-greedy algorithm (0 <= epsilon <= 1)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # initialize true expected value of all actions with N(0,1) distribution,\n",
    "        # initialize the estimations with desired initial value (typically 0),\n",
    "        # and set all action counts to zero\n",
    "        for i in range(self.k):\n",
    "            self.qTrue.append(np.random.randn() + trueReward)\n",
    "            self.qEst[i] = initial\n",
    "            self.actionCount.append(0)\n",
    "\n",
    "        # Determine the best action from the true expected values of all actions\n",
    "        self.bestAction = np.argmax(self.qTrue)\n",
    "\n",
    "    # get an action for this bandit, explore or exploit?\n",
    "    def getAction(self):\n",
    "        # explore\n",
    "        if self.epsilon > 0:\n",
    "            # np.random.binomial(n,p) is the result of flipping a coin n times\n",
    "            #   probability of heads (1) =  p\n",
    "            #   probability of tails (0) = 1-p\n",
    "            if np.random.binomial(1, self.epsilon) == 1:\n",
    "                # For an exploratory (random) action, shuffle the indices array\n",
    "                # and return the first index in the shuffled array.\n",
    "                np.random.shuffle(self.indices)\n",
    "                return self.indices[0]\n",
    "\n",
    "        # exploit\n",
    "        if self.UCBParam is not None:\n",
    "            # use Upper-Confidence-Bound (UCB) algorithm to select action\n",
    "            UCBEst = self.qEst + \\\n",
    "                     self.UCBParam * np.sqrt(np.log(self.time + 1) / (np.asarray(self.actionCount) + 1))\n",
    "            return np.argmax(UCBEst)\n",
    "        if self.gradient:\n",
    "            # use gradient based bandit algorithm\n",
    "            expEst = np.exp(self.qEst)\n",
    "            self.actionProb = expEst / np.sum(expEst)\n",
    "            return np.random.choice(self.indices, p=self.actionProb)\n",
    "        return np.argmax(self.qEst)\n",
    "\n",
    "    # take an action, update estimation for this action\n",
    "    def takeAction(self, action):\n",
    "        # generate the reward under N(real reward, 1)\n",
    "        reward = np.random.randn() + self.qTrue[action]\n",
    "        self.time += 1\n",
    "        self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time\n",
    "        self.actionCount[action] += 1\n",
    "\n",
    "        if self.sampleAverages:\n",
    "            # update estimation using sample averages\n",
    "            self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])\n",
    "        elif self.gradient:\n",
    "            oneHot = np.zeros(self.k)\n",
    "            oneHot[action] = 1\n",
    "            if self.gradientBaseline:\n",
    "                baseline = self.averageReward\n",
    "            else:\n",
    "                baseline = 0\n",
    "            self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)\n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.qEst[action] += 0.1 * (reward - self.qEst[action])\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **run** the cell below to create the `epsilonGreedy` method, which will simulate the $k$-Armed Bandit Problem multiple times for a prescribed number of timesteps and prescribed list of $\\epsilon$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for figure 2.2 from Sutton and Barto\n",
    "def epsilonGreedy(nBandits, time, k=10, epsilons = [0.1, 0.01, 0]):\n",
    "    # @nBandits: (integer) The number of k-Armed Bandit Problems (kABP) to simulate\n",
    "    # @time: (integer) The number of timesteps to simulate in each kABP\n",
    "    # @k: (integer) The number of arms (actions) for each kABP\n",
    "    # @epsilons: (list of floats) Value(s) of epsilon for epsilon greedy selection\n",
    "    \n",
    "    # Initialize the figure counter to zero\n",
    "    figureIndex = 0\n",
    "\n",
    "    # Initialize a counter for each timestep and for each epsilon to keep track of \n",
    "    # the number of times the best action was chosen for each timestep\n",
    "    bestActionCounts = [np.zeros(time, dtype='float') for _ in range(len(epsilons))]\n",
    "    \n",
    "    # Initialize an array for the average reward at each timestep for each epsilon\n",
    "    averageRewards = [np.zeros(time, dtype='float') for _ in range(len(epsilons))]\n",
    "    \n",
    "    # Initialize empty lists, one for each epsilon, that will contain all Bandit objects\n",
    "    bandits = [list() for _ in range(len(epsilons))]\n",
    "    \n",
    "    # Create all nBandits Bandit objects for each value of epsilon,\n",
    "    # initialize them with k actions, and the correct epsilon value.\n",
    "    # Set sampleAverages to True - we are calculating qEst from the sample averages\n",
    "    for i, (eps, bandit) in enumerate(zip(epsilons,bandits)):\n",
    "        for _ in range(nBandits):\n",
    "            bandit.append(Bandit(kArm=k, epsilon=eps, sampleAverages=True))\n",
    "    \n",
    "    # Now iterate through each Bandit object and take actions for the prescribed\n",
    "    # number of timesteps. Update the rewards and average rewards, and increment\n",
    "    # the best action counter if the best action was selected.\n",
    "    for epsInd in range(len(epsilons)):\n",
    "        for i in range(nBandits):\n",
    "            for t in range(time):\n",
    "                action = bandits[epsInd][i].getAction()\n",
    "                reward = bandits[epsInd][i].takeAction(action)\n",
    "                averageRewards[epsInd][t] += reward\n",
    "                if action == bandits[epsInd][i].bestAction:\n",
    "                    bestActionCounts[epsInd][t] += 1\n",
    "    \n",
    "    # First figure: % of times at each timestep the optimal action was chosen.\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for eps, counts in zip(epsilons, bestActionCounts):\n",
    "        counts /= (nBandits*0.01)\n",
    "        plt.plot(counts, label='$\\epsilon$ = '+str(eps))\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('% Optimal Action')\n",
    "    plt.ylim([0,100])\n",
    "    plt.title('% Optimal Action\\n({}-Armed Bandit, Average Over {} Runs)'.format(k, nBandits))\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Second figure: average reward at each timestep for each epsilon.\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for eps, rewards in zip(epsilons, averageRewards):\n",
    "        rewards /= nBandits\n",
    "        plt.plot(rewards, label='$\\epsilon$ = '+str(eps))\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Average Reward\\n({}-Armed Bandit, Average Over {} Runs)'.format(k, nBandits))\n",
    "    plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **run** the cell below to generate a plot of the percentage of times the greedy and $\\epsilon$-greedy action selection methods select the optimal action for each timestep. You should see that the greedy action selection method quickly plateaus, so on average it's finding the optimal action fewer than 40% of the time! The $\\epsilon$-greedy methods improve over time: with a larger $\\epsilon$ of 0.1, the improvement is more rapid, but the upper bound on the percentage of optimal actions for very long times is around 91% in this case. With a smaller $\\epsilon$ of 0.01, the improvement at each timestep is slower, but the upper bound on the optimal percentage is closer to 99%. Can you figure out why both of these optimal percentages are bounded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "epsilonGreedy(2000,1000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under Construction...\n",
    "\n",
    "In the future, I'd like to add to this notebook to contain a discussion of the rest of the Chapter 2 content from the Sutton and Barto text. For now, I'm including Shangtong Zhang's code as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for figure 2.3\n",
    "def optimisticInitialValues(nBandits, time):\n",
    "    bandits = [[], []]\n",
    "    bestActionCounts = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))]\n",
    "    bandits[0] = [Bandit(epsilon=0, initial=5, stepSize=0.1) for _ in range(0, nBandits)]\n",
    "    bandits[1] = [Bandit(epsilon=0.1, initial=0, stepSize=0.1) for _ in range(0, nBandits)]\n",
    "    for banditInd in range(0, len(bandits)):\n",
    "        for i in range(0, nBandits):\n",
    "            for t in range(0, time):\n",
    "                action = bandits[banditInd][i].getAction()\n",
    "                bandits[banditInd][i].takeAction(action)\n",
    "                if action == bandits[banditInd][i].bestAction:\n",
    "                    bestActionCounts[banditInd][t] += 1\n",
    "    bestActionCounts[0] /= nBandits\n",
    "    bestActionCounts[1] /= nBandits\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    plt.plot(bestActionCounts[0], label='epsilon = 0, q = 5')\n",
    "    plt.plot(bestActionCounts[1], label='epsilon = 0.1, q = 0')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "\n",
    "# for figure 2.4\n",
    "def ucb(nBandits, time):\n",
    "    bandits = [[], []]\n",
    "    averageRewards = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))]\n",
    "    bandits[0] = [Bandit(epsilon=0, stepSize=0.1, UCBParam=2) for _ in range(0, nBandits)]\n",
    "    bandits[1] = [Bandit(epsilon=0.1, stepSize=0.1) for _ in range(0, nBandits)]\n",
    "    for banditInd in range(0, len(bandits)):\n",
    "        for i in range(0, nBandits):\n",
    "            for t in range(0, time):\n",
    "                action = bandits[banditInd][i].getAction()\n",
    "                reward = bandits[banditInd][i].takeAction(action)\n",
    "                averageRewards[banditInd][t] += reward\n",
    "    averageRewards[0] /= nBandits\n",
    "    averageRewards[1] /= nBandits\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    plt.plot(averageRewards[0], label='UCB c = 2')\n",
    "    plt.plot(averageRewards[1], label='epsilon greedy epsilon = 0.1')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "\n",
    "# for figure 2.5\n",
    "def gradientBandit(nBandits, time):\n",
    "    bandits =[[], [], [], []]\n",
    "    bandits[0] = [Bandit(gradient=True, stepSize=0.1, gradientBaseline=True, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bandits[1] = [Bandit(gradient=True, stepSize=0.1, gradientBaseline=False, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bandits[2] = [Bandit(gradient=True, stepSize=0.4, gradientBaseline=True, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bandits[3] = [Bandit(gradient=True, stepSize=0.4, gradientBaseline=False, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bestActionCounts = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))]\n",
    "    for banditInd in range(0, len(bandits)):\n",
    "        for i in range(0, nBandits):\n",
    "            for t in range(0, time):\n",
    "                action = bandits[banditInd][i].getAction()\n",
    "                bandits[banditInd][i].takeAction(action)\n",
    "                if action == bandits[banditInd][i].bestAction:\n",
    "                    bestActionCounts[banditInd][t] += 1\n",
    "    for counts in bestActionCounts:\n",
    "        counts /= nBandits\n",
    "    labels = ['alpha = 0.1, with baseline',\n",
    "              'alpha = 0.1, without baseline',\n",
    "              'alpha = 0.4, with baseline',\n",
    "              'alpha = 0.4, without baseline']\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for i in range(0, len(bandits)):\n",
    "        plt.plot(bestActionCounts[i], label=labels[i])\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% Optimal action')\n",
    "    plt.legend()\n",
    "\n",
    "# Figure 2.6\n",
    "def figure2_6(nBandits, time):\n",
    "    labels = ['epsilon-greedy', 'gradient bandit',\n",
    "              'UCB', 'optimistic initialization']\n",
    "    generators = [lambda epsilon: Bandit(epsilon=epsilon, sampleAverages=True),\n",
    "                  lambda alpha: Bandit(gradient=True, stepSize=alpha, gradientBaseline=True),\n",
    "                  lambda coef: Bandit(epsilon=0, stepSize=0.1, UCBParam=coef),\n",
    "                  lambda initial: Bandit(epsilon=0, initial=initial, stepSize=0.1)]\n",
    "    parameters = [np.arange(-7, -1),\n",
    "                  np.arange(-5, 2),\n",
    "                  np.arange(-4, 3),\n",
    "                  np.arange(-2, 3)]\n",
    "    averageRewards = []\n",
    "    for i in range(len(labels)):\n",
    "        averageRewards.append([])\n",
    "        for param_ in parameters[i]:\n",
    "            param = pow(2, param_)\n",
    "            reward = 0.0\n",
    "            for banditIndex in range(nBandits):\n",
    "                bandit = generators[i](param)\n",
    "                print labels[i], 'parameter:', param, str(banditIndex) + 'th bandit'\n",
    "                for step in range(time):\n",
    "                    action = bandit.getAction()\n",
    "                    reward += bandit.takeAction(action)\n",
    "            reward /= nBandits * time\n",
    "            averageRewards[-1].append(reward)\n",
    "\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for i in range(len(labels)):\n",
    "        plt.plot(parameters[i], averageRewards[i], label=labels[i])\n",
    "    plt.xlabel('Parameter(2^x)')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "# epsilonGreedy(200, 1000)\n",
    "# optimisticInitialValues(200, 1000)\n",
    "# ucb(1000, 1000)\n",
    "# gradientBandit(200, 1000)\n",
    "# figure2_6(200, 1000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
