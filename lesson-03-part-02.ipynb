{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 03: Building and evaluating models with `sklearn`\n",
    "# Part 02: Predictions, accuracy, and validation\n",
    "\n",
    "## Objectives\n",
    "  - Use the `sklearn` library to build a predictive `DecisionTreeClassifier` model for the Titanic Survival Dataset.\n",
    "  - Compute the accuracy of a model on both the training and validation (testing) data.\n",
    "  - Adjust hyperparameters (e.g. `max_depth`) to see the effects on model accuracy.\n",
    "  - Visualize the model to understand how it makes decisions, and introduce the Gini impurity.\n",
    "  \n",
    "## Prerequisites\n",
    "  - You should have the following python packages installed:\n",
    "    - [matplotlib](http://matplotlib.org/index.html)\n",
    "    - [numpy](http://www.scipy.org/scipylib/download.html)\n",
    "    - [pandas](http://pandas.pydata.org/getpandas.html)\n",
    "    - [sklearn](http://scikit-learn.org/stable/install.html)\n",
    "  - You should have completed part 01 of this lesson to create the cleaned data sets. You can find all lessons in the [ConnectIntensive repo](https://github.com/nickypie/ConnectIntensive).\n",
    "  - Optional prerequisites for visualizing the decision trees.\n",
    "    - Install [graphviz](http://graphviz.org/), graph visualization software.\n",
    "      - If you use [homebrew](http://brew.sh/), there's [a brew formula available for graphviz](http://brewformulas.org/Graphviz)\n",
    "    - Install [pydotplus](https://pypi.python.org/pypi/pydotplus), a Python interface to Graphviz's Dot language\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "  - This lesson is adapted from part 2 of Thomas Corcoran's excellent [`sklearn` tutorial](https://github.com/tccorcoran/Connect/tree/master/sklearn-tutorial). Thank you Thomas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "As usual, we start by importing some useful libraries and modules. Don't worry if you get a warning message when importing `matplotlib` -- it just needs to build the font cache, and the warning is just to alert you that this may take a while the first time the cell is run.\n",
    "\n",
    "**Run** the cell below (**click** the cell to highlight it, then press **shift + enter** or **shift + return**) to import useful libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('ggplot')\n",
    "    print(\"Successfully imported matplotlib.pyplot! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import matplotlib.pyplot!\")\n",
    "    \n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    print(\"Could not import numpy!\")\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    print(\"Successfully imported display from IPython.display!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display\")\n",
    "    \n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"Successfully imported sklearn! (Version {})\".format(sklearn.__version__))\n",
    "    skversion = int(sklearn.__version__[2:4])\n",
    "except ImportError:\n",
    "    print(\"Could not import sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the preprocessed data\n",
    "In lesson-03-part-01.ipynb, we prepared the Titanic data for model building:\n",
    "  - We explored the training data set to determine the predictive features to use in our models.\n",
    "    - Features like `'Embarked'` and `'Pclass'` appeared to have useful predictive power.\n",
    "    - Features like `'PassengerId'` and `'Ticket'` seemed less helpful.\n",
    "  - We preprocessed the data to make it compatible with `sklearn`.\n",
    "    - Missing values in the `'Age'` feature were imputed.\n",
    "    - The categorical variables `'Sex'` and `'Embarked'` were changed to numeric features.\n",
    "Now we want to use the preprocessed data to build a predictive model.\n",
    "\n",
    "**Run** the cell below (**click** on the cell to highlight it, then press **shift + enter** or **shift + return** to run it) to read the preprocessed training and testing data into `pandas` `DataFrame` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"lesson-03-data/titanic_train_cleaned.csv\")\n",
    "test_df  = pd.read_csv(\"lesson-03-data/titanic_test_cleaned.csv\")\n",
    "print(\"Titanic data sets loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further dividing the data\n",
    "The testing Titanic Survival data set from Kaggle intentionally does not have the `'Survived'` feature. So if we want to train and validate our model, we will need to further split our training data. In practice, this is often called training, validation, and testing sets.\n",
    "  - **Training set:** A set of examples used for machine learning, that is to fit the parameters (*i.e.*, weights) of the classifier.\n",
    "  - **Validation set:** A set of examples used to tune the hyperparameters (*i.e.*, architecture, not weights) of a classifier, for example to choose the maximum depth of a decision tree, or the number of hidden layers in a neural network.\n",
    "  - **Test set:** A set of examples used only *once*. This assesses the performance (generalization) of the fully-specified classifier (once all hyperparameters have been specified).\n",
    "\n",
    "So you can think of the three groups of data this way: you build a collection of models from the same training data set. The models might all be slightly different (*e.g.* decision trees of maximum depths of 1, 2, 3, 4, ...). You then apply each of these models to the validation data set. Based on how well each of these models performs, you may select an optimal depth, which was the adjustable *hyperparameter* for the decision tree classifier. Finally, once the model and all of its adjustable parameters have been decided upon, this model can be applied to the testing data to see how well it generalizes to unknown data.\n",
    "\n",
    "For more on testing vs. validation data sets, you can consult the [test set wikipedia article](https://en.wikipedia.org/wiki/Test_set) or [this Quora post on training and testing data](https://www.quora.com/What-is-a-training-data-set-test-data-set-in-machine-learning-What-are-the-rules-for-selecting-them).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `train_test_split`\n",
    "The latest version of the library `sklearn` has the module `model_selection`, which contains [the method `train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). We can use this method to split `train_df` further into a training and a validation set. The arguments that we need to pass to `train_test_split()` are:\n",
    "  - `X` & `y`: Arrays. These can be `pandas` `DataFrame` or `Series` objects.\n",
    "  - `test_size`: A float with the proportion of data to put into the test set. *e.g.* `test_size = 0.1` would put one in every ten instances into the test set.\n",
    "  - `random_state`: The pseudo-random number generator state used for random sampling. For a given value of `random_state`, the method will partition the data set exactly the same way each time, which is useful for debugging.\n",
    "\n",
    "**Run** the cell below to create the `DataFrame` object `X` and the `Series` object `y` from the desired features from `train_df`. Then use `train_test_split()` with a `random_state` to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Starting with scikit-learn version 0.18, the model_selection module replaces the cross_validation module,\n",
    "# so we should import train_test_split from the appropriate module depending on the version number.\n",
    "if skversion >= 18:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "else:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Make a list of the desired feature names for model building\n",
    "desired_features = ['Pclass', 'Sex', 'Age','SibSp','Parch', 'Cherbourg','Queenstown','Southampton']\n",
    "\n",
    "# X is our pandas DataFrame object with the features from which we will predict the 'Survived' feature.\n",
    "X = pd.DataFrame(train_df[desired_features])\n",
    "\n",
    "# y is our pandas Series object with the 'Survived' feature to be predicted.\n",
    "y = pd.Series(train_df['Survived'])\n",
    "\n",
    "# Split the data into training and validation (test) data sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Take a look at the first few rows of the training features and classes\n",
    "display(X_train.head())\n",
    "display(y_train.head())\n",
    "\n",
    "# Verify that the data sets were split 80% training and 20% testing\n",
    "print(\"The original data ({} instances) was split into training ({} instances) and testing ({} instances) data sets\".\\\n",
    "     format(len(X),len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the indices on `X_train.head()` and `y_train.head()` -- they match, as expected, but they're shuffled, which may be unexpected! Also, the print statement above shows us that `train_test_split()` is partitioning the data based on the given `test_size` argument. Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree Classifier\n",
    "For supervised learning problems, the model building `sklearn` workflow is pretty similar, regardless of the type of classifier you'd like to build:\n",
    "  1. **Create** a classifier object.\n",
    "  2. **Train** the classifier on the training data set.\n",
    "  3. **Predict** with the classifier on the validation (test) data set.\n",
    "  4. **Assess** the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "\n",
    "Let's try it here! Let's build a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) of `max_depth` 1. We will need to import `DecisionTreeClassifier` and `accuracy_score` from the appropriate `sklearn` modules.\n",
    "\n",
    "**Run** the cell below to **create** a Decision Tree Classifier, **train** it on the training data, **predict** class labels for the validation (test) data set, and **assess** the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier and accuracy_score from the appropriate sklearn modules\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. CREATE the classifier object... in this example, we call it clf1\n",
    "clf1 = DecisionTreeClassifier(random_state=0, max_depth=1)\n",
    "\n",
    "# 2. TRAIN the classifier object using the method .fit()\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "# 3. PREDICT labels for the validation (test) set using the method .predict()\n",
    "y_pred_train = clf1.predict(X_train)\n",
    "y_pred_test  = clf1.predict(X_test)\n",
    "\n",
    "# 4. ASSESS the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "print(\"The model with max_depth of {} has an accuracy of {:.1f}% on the training data, and {:.1f}% on the testing data\".\\\n",
    "      format(1,\\\n",
    "             100.0*accuracy_score(y_pred_train,y_train),\\\n",
    "             100.0*accuracy_score(y_pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good, right? Our decision tree of `max_depth` 1 correctly classified nearly 80% of the testing data! Let's look under the hood to see what criteria the decision tree classifier used to create its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Decision Tree (Optional)\n",
    "Some advantages to a decision tree classifier are that the classification process is simple to understand and interpret, and the decision tree can be easily visualized. Looking into the `sklearn` [documentation on Decision Trees](http://scikit-learn.org/stable/modules/tree.html), we see that there is a method, `export_graphviz`, where we can visualize the decision tree in [Graphviz format](http://www.graphviz.org/). Getting this working on your machine may be a little tricky, because there's an additional application and Python module needed, but I think the payoff is worth it.\n",
    "  1. You need to install the [Graphviz](http://www.graphviz.org/) application. Note that [`pip install graphviz`](https://pypi.python.org/pypi/graphviz) is **not** sufficient. It does not install the Graphviz application.\n",
    "  2. You need to install [the module `pydotplus`](https://pypi.python.org/pypi/pydotplus)\n",
    "  \n",
    "Once you've completed the installation prerequisites above, **run** the cell below to visualize the Decision Tree Classifier you created above with `max_depth` 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = export_graphviz(clf1, out_file=None, \n",
    "                         feature_names=desired_features,  \n",
    "                         class_names=['Died','Survived'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the decision tree drawn to the Jupyter Notebook! At the top is the **root node**, and we can see the question that the Decision Tree Classifier will use to make a decision. It will query the `'Sex'` feature, asking whether or not it is less than 0.5. If true, it moves along the left branch to the left leaf node, otherwise it moves to the right leaf node. Remember in the preprocessing step, we transformed the `'Sex'` feature into a numeric value of 0 for `'female'` passengers and 1 for `'male'` passengers. Let's describe some of the aspects of the graph:\n",
    "\n",
    "  - **gini:** The [Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity), or a measure of how often samples would be mislabeled at a given node. For a class with two possible labels...\n",
    "    - The maximum possible value of `gini` is 0.5, when there is equal representation from both classes at the node.\n",
    "    - The minimum possible value of `gini` is 0, when the node is purely a single class.\n",
    "  - **samples:** the number of instances or inputs that reach that node of the tree. \n",
    "    - All 712 instances in the training set will pass through the root node, so `samples = 712` for the root node.\n",
    "    - There are 240 females in the training set, so `samples = 240` for the females leaf node.\n",
    "    - There are 472 males in the training set, so `samples = 472` for the males leaf node.\n",
    "  - **value:** an array of length `n`, where `n` is the total number of distinct classes. Here, our classes were 0 and 1, indicating a passenger \"Died\" or \"Survived\".\n",
    "    - Of the entire training set, 440 passengers Died and 272 passengers Survived, so `value = [440,272]` for the root node.\n",
    "    - Of females in the training set, 60 passengers Died and 180 passengers Survived, so `value = [60,180]` for the females leaf node.\n",
    "    - Of males in the training set, 380 passengers Died and 92 passengers Survived, so `value = [380,92]` for the males leaf node.\n",
    "  - **class:** the best guess the Decision Tree Classifier can make at each node\n",
    "    - At the root node, more passengers Died (440) than Survived (272), so the best guess it can make here is `class = Died`\n",
    "    - At the females leaf node, fewer passengers Died (60) than Survived (180), so the best guess it can make here is `class = Survived`\n",
    "    - At the males leaf node, more passengers Died (380) than Survived (92), so the best guess it can make here is `class = Died`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with hyperparameters\n",
    "Hyperparameters of a model are explained pretty succinctly [in this Quora post](https://www.quora.com/What-are-hyperparameters-in-machine-learning). Basically, they are parameters that cannot be \"learned\" during the model training step. For example, you specify when you create the DecisionTreeClassifier object what the maximum allowed depth of the decision tree will be. That `max_depth` argument is a **hyperparameter**. During the model validation phase, a machine learnist often adjusts the hyperparamters and assesses how those adjustments improve the model on the training or testing set. Let's try it here!\n",
    "\n",
    "**Run** the cell below to create a `DecisionTreeClassifier` object with a `max_depth` of 3. Does the model perform better or worse on the testing data than the original model with `max_depth` of 1? ...on the validation (test) data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. CREATE the classifier object... in this example, we call it clf3\n",
    "clf3 = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "\n",
    "# 2. TRAIN the classifier object using the method .fit()\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "# 3. PREDICT labels for the validation (test) set using the method .predict()\n",
    "y_pred_train = clf3.predict(X_train)\n",
    "y_pred_test  = clf3.predict(X_test)\n",
    "\n",
    "# 4. ASSESS the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "print(\"The model with max_depth of {} has an accuracy of {:.1f}% on the training data, and {:.1f}% on the testing data\".\\\n",
    "      format(3,\\\n",
    "             100.0*accuracy_score(y_pred_train,y_train),\\\n",
    "             100.0*accuracy_score(y_pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run** the cell below to visualize the Decision Tree Classifier model with `max_depth` of 3.\n",
    "  - Can you follow the decision tree?\n",
    "  - At which leaf node(s) are the predictions the strongest (lowest gini impurity)?\n",
    "  - At which leaf node(s) are the predictions the weakest (highest gini impurity)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(clf3, out_file=None, \n",
    "                         feature_names=desired_features,  \n",
    "                         class_names=['Died','Survived'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware overfitting!\n",
    "We could continue increasing `max_depth` and the accuracy on the training set should increase each time. However, the accuracy on the validation set will not necessarily improve with increasing tree depth. Why is this? As you increase the `max_depth` hyperparameter, your model becomes more strongly tuned to the training data set and you run the risk of **overfitting** your training data. This is a classic instnace of the bias vs. variance tradeoff, which is described wonderfully [in this blogpost](http://scott.fortmann-roe.com/docs/BiasVariance.html) by Scott Fortmann-Roe.\n",
    "\n",
    "**Run** the cell below to see how well the Decision Tree Classifier works when we remove restrictions on the maximum depth -- the classifier will continue to split data until it runs out of possible features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. CREATE the classifier object... in this example, we call it clf\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth = None)\n",
    "\n",
    "# 2. TRAIN the classifier object using the method .fit()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. PREDICT labels for the validation (test) set using the method .predict()\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test  = clf.predict(X_test)\n",
    "\n",
    "# 4. ASSESS the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "print(\"The model with max_depth of {} has an accuracy of {:.1f}% on the training data, and {:.1f}% on the testing data\".\\\n",
    "      format(\"None\",\\\n",
    "             100.0*accuracy_score(y_pred_train,y_train),\\\n",
    "             100.0*accuracy_score(y_pred_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy on the training data is great, but our accuracy on the testing data didn't improve from the last model. In fact, it decreased! The model became too sensitive to trends in the training data that are not representative of the population as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could we do next?\n",
    "Now that we've seen how to create, train, predict, and assess a classifier using `sklearn`, we are ready to try more advanced techniques:\n",
    "  1. [More types of cross-validation, including k-fold cross validation.](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "  2. [Make validation curves and learning curves to see how well our models perform with different hyperparameters or training set sizes.](http://scikit-learn.org/stable/modules/learning_curve.html)\n",
    "  3. [Perform an exhaustive grid search to tune and find the best possible hyperparameters](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "  \n",
    "I hope you're excited to try all these! If you'd like more practice right away, check out Thomas' [sklearn tutorial on GitHub](https://github.com/tccorcoran/Connect/tree/master/sklearn-tutorial)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
